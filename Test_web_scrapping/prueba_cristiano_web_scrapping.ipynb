{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "342aeadb-9609-4c23-a800-3bf3d8184e57",
   "metadata": {},
   "source": [
    "## Todo el contenido JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319fe458-79ad-4fc3-998d-f0652b586374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "url = \"https://es.wikipedia.org/w/api.php\"\n",
    "\n",
    "# Función para obtener y limpiar el contenido de una sección específica\n",
    "def obtener_contenido_seccion(seccion_numero, seccion_nombre):\n",
    "    params = {\n",
    "        \"action\": \"parse\",\n",
    "        \"page\": \"Cristiano Ronaldo\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"text\",\n",
    "        \"section\": seccion_numero\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        content_html = data[\"parse\"][\"text\"][\"*\"]\n",
    "        soup = BeautifulSoup(content_html, \"html.parser\")\n",
    "        \n",
    "        # Elimina referencias y notas adicionales\n",
    "        for ref in soup.find_all(['sup', 'span', 'a'], {'class': ['reference', 'mw-editsection']}):\n",
    "            ref.decompose()\n",
    "        \n",
    "        # Convierte el contenido a texto limpio y unifica en un solo párrafo\n",
    "        clean_text = \" \".join(soup.get_text(separator=\"\\n\").split())\n",
    "        \n",
    "        # Retorna el contenido en un formato estructurado\n",
    "        return {\"seccion\": seccion_nombre, \"contenido\": clean_text}\n",
    "    else:\n",
    "        print(f\"Error en la solicitud: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Obtener el título de la página\n",
    "params_title = {\n",
    "    \"action\": \"query\",\n",
    "    \"titles\": \"Cristiano Ronaldo\",\n",
    "    \"format\": \"json\"\n",
    "}\n",
    "response_title = requests.get(url, params=params_title)\n",
    "if response_title.status_code == 200:\n",
    "    data_title = response_title.json()\n",
    "    pages = data_title[\"query\"][\"pages\"]\n",
    "    page_id = next(iter(pages))\n",
    "    titulo_pagina = pages[page_id][\"title\"]\n",
    "else:\n",
    "    titulo_pagina = \"Cristiano Ronaldo\"\n",
    "\n",
    "# Obtener la lista de secciones\n",
    "params_sections = {\n",
    "    \"action\": \"parse\",\n",
    "    \"page\": \"Cristiano Ronaldo\",\n",
    "    \"format\": \"json\",\n",
    "    \"prop\": \"sections\"\n",
    "}\n",
    "response_sections = requests.get(url, params=params_sections)\n",
    "if response_sections.status_code == 200:\n",
    "    data_sections = response_sections.json()\n",
    "    secciones = data_sections[\"parse\"][\"sections\"]\n",
    "else:\n",
    "    print(f\"Error al obtener las secciones: {response_sections.status_code}\")\n",
    "    secciones = []\n",
    "\n",
    "# Lista de secciones a excluir\n",
    "secciones_excluir = [\"Véase también\", \"Filmografía\", \"Notas\", \"Referencias\", \"Bibliografía\", \"Enlaces externos\"]\n",
    "\n",
    "# Iterar sobre todas las secciones y obtener su contenido, excluyendo las especificadas\n",
    "contenido_secciones = []\n",
    "for seccion in secciones:\n",
    "    seccion_numero = seccion[\"index\"]\n",
    "    seccion_nombre = seccion[\"line\"]\n",
    "    if seccion_nombre not in secciones_excluir:\n",
    "        contenido = obtener_contenido_seccion(seccion_numero, seccion_nombre)\n",
    "        if contenido:\n",
    "            contenido_secciones.append(contenido)\n",
    "\n",
    "# Estructura el contenido en un JSON con todas las secciones e incluye el título\n",
    "json_data2 = {\n",
    "    \"titulo\": titulo_pagina,\n",
    "    \"secciones\": contenido_secciones\n",
    "}\n",
    "\n",
    "# Convierte el diccionario a JSON\n",
    "json_output2 = json.dumps(json_data, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Imprime o guarda el JSON estructurado\n",
    "print(json_output2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b3f4bd-0d6b-4bde-bc9b-e8f4551c4009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el diccionario en un archivo JSON local\n",
    "with open('cristiano_ronaldo_test_f.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_data2, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"El archivo 'cristiano_ronaldo_test_f.json' ha sido guardado exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd13b54-b4f8-420b-bdfd-db7cc4b0e513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Especificar el nombre del bucket y el nombre del archivo en S3\n",
    "bucket_name = 'leonardo-devops-days-staging'  # Reemplaza con el nombre de tu bucket\n",
    "s3_file_name = 'cristiano_ronaldo_test_f.json'\n",
    "\n",
    "# Subir el JSON al bucket de S3\n",
    "try:\n",
    "    s3.put_object(Bucket=bucket_name, Key=s3_file_name, Body=json_output, ContentType='application/json')\n",
    "    print(f\"El archivo '{s3_file_name}' ha sido subido exitosamente al bucket '{bucket_name}'.\")\n",
    "except NoCredentialsError:\n",
    "    print(\"Error: No se encontraron las credenciales de AWS.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error al subir el archivo: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9b4eab-9c13-415e-9c93-15139eb6f8c3",
   "metadata": {},
   "source": [
    "## secciones particulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bef4440-e8d1-48b3-b940-25de3dd213ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para obtener y limpiar el contenido de una sección específica\n",
    "def obtener_contenido_seccion(seccion_numero, seccion_nombre):\n",
    "    params = {\n",
    "        \"action\": \"parse\",\n",
    "        \"page\": \"Cristiano Ronaldo\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"text\",\n",
    "        \"section\": seccion_numero\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        content_html = data[\"parse\"][\"text\"][\"*\"]\n",
    "        soup = BeautifulSoup(content_html, \"html.parser\")\n",
    "        \n",
    "        # Elimina referencias y notas adicionales\n",
    "        for ref in soup.find_all(['sup', 'span', 'a'], {'class': ['reference', 'mw-editsection']}):\n",
    "            ref.decompose()\n",
    "        \n",
    "        # Convierte el contenido a texto limpio y unifica en un solo párrafo\n",
    "        clean_text = \" \".join(soup.get_text(separator=\"\\n\").split())\n",
    "        \n",
    "        # Retorna el contenido en un formato estructurado\n",
    "        return {\"club\": seccion_nombre, \"historia\": clean_text}\n",
    "    else:\n",
    "        print(f\"Error en la solicitud: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Obtiene el contenido de la sección \"Sporting C. P.\"\n",
    "contenido_sporting = obtener_contenido_seccion(3, \"Sporting C. P.\")\n",
    "# Obtiene el contenido de la sección \"Manchester United F. C.\"\n",
    "contenido_manchester = obtener_contenido_seccion(4, \"Manchester United F. C.\")\n",
    "\n",
    "# Estructura el contenido en un JSON con ambas secciones\n",
    "json_data = {\n",
    "    \"sections\": [contenido_sporting, contenido_manchester]\n",
    "}\n",
    "\n",
    "# Convierte el diccionario a JSON\n",
    "json_output = json.dumps(json_data, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Imprime o guarda el JSON estructurado\n",
    "print(json_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f49a52-44e7-4ace-b200-d80616148a2f",
   "metadata": {},
   "source": [
    "## Encabezados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d785eef-5cb9-4130-90ec-225a171b8c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Paso 1: Obtén el número de sección de \"Carrera en clubes\"\n",
    "url = \"https://es.wikipedia.org/w/api.php\"\n",
    "params = {\n",
    "    \"action\": \"parse\",\n",
    "    \"page\": \"Cristiano Ronaldo\",\n",
    "    \"format\": \"json\",\n",
    "    \"prop\": \"sections\"  # Solicitamos solo las secciones\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    \n",
    "    # Encuentra la sección de \"Carrera en clubes\"\n",
    "    section_number = None\n",
    "    for section in data[\"parse\"][\"sections\"]:\n",
    "        if section[\"line\"] == \"Carrera en clubes\":\n",
    "            section_number = section[\"index\"]\n",
    "            break\n",
    "\n",
    "    # Si encontramos el número de sección, hacemos la solicitud para obtener su contenido\n",
    "    if section_number:\n",
    "        params = {\n",
    "            \"action\": \"parse\",\n",
    "            \"page\": \"Cristiano Ronaldo\",\n",
    "            \"format\": \"json\",\n",
    "            \"prop\": \"text\",\n",
    "            \"section\": section_number\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            content_html = data[\"parse\"][\"text\"][\"*\"]\n",
    "            \n",
    "            # Paso 2: Analiza el HTML y estructura los datos\n",
    "            soup = BeautifulSoup(content_html, \"html.parser\")\n",
    "            structured_data = {\"Carrera en clubes\": {}}\n",
    "            \n",
    "            # Encuentra todos los subtítulos (h4) en la sección \"Carrera en clubes\"\n",
    "            for header in soup.find_all(\"h4\"):\n",
    "                section_title = header.get_text(strip=True)\n",
    "                paragraphs = []\n",
    "                \n",
    "                # Extrae los párrafos asociados a esta subsección\n",
    "                for sibling in header.find_next_siblings():\n",
    "                    if sibling.name == \"h4\":\n",
    "                        break  # Detente en la siguiente subsección\n",
    "                    if sibling.name == \"p\":\n",
    "                        paragraphs.append(sibling.get_text(strip=True))\n",
    "\n",
    "                # Agrega la subsección y su contenido al diccionario\n",
    "                structured_data[\"Carrera en clubes\"][section_title] = \" \".join(paragraphs)\n",
    "\n",
    "            # Convierte el diccionario en JSON y lo muestra\n",
    "            structured_data_json = json.dumps(structured_data, indent=4, ensure_ascii=False)\n",
    "            print(structured_data_json)\n",
    "        else:\n",
    "            print(\"Error al obtener el contenido de la sección.\")\n",
    "    else:\n",
    "        print(\"No se encontró la sección 'Carrera en clubes'.\")\n",
    "else:\n",
    "    print(f\"Error al obtener las secciones: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f065a3d6-2f84-4c19-9d55-f984d0822b42",
   "metadata": {},
   "source": [
    "## section 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d287e6-5b9d-40f4-a5a7-e1d3bfd96d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Configura los parámetros para la solicitud\n",
    "url = \"https://es.wikipedia.org/w/api.php\"\n",
    "params = {\n",
    "    \"action\": \"parse\",              # Utiliza la acción \"parse\" para obtener contenido de secciones\n",
    "    \"page\": \"Cristiano Ronaldo\",    # Página de la cual obtener información\n",
    "    \"format\": \"json\",               # Formato de respuesta\n",
    "    \"prop\": \"text\",                 # Propiedad para obtener el contenido en HTML\n",
    "    \"section\": 3                    # Número de la sección (3 corresponde a \"Carrera en clubes\" en esta página)\n",
    "}\n",
    "\n",
    "# Realiza la solicitud\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Verifica el código de respuesta\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    content_html = data[\"parse\"][\"text\"][\"*\"]  # Extrae el HTML de la sección\n",
    "    print(content_html)\n",
    "else:\n",
    "    print(f\"Error en la solicitud: {response.status_code}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
